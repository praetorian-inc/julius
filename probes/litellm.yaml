# probes/litellm.yaml
# Reference: https://docs.litellm.ai/docs/proxy/health
name: litellm
description: LiteLLM Proxy - unified API gateway for multiple LLM providers
category: gateway
port_hint: 4000
specificity: 85
api_docs: https://docs.litellm.ai/docs/

requests:
  - type: http
    path: /health
    method: GET
    match:
      - type: status
        value: 200
      - type: body.contains
        value: '"healthy_endpoints"'
      - type: body.contains
        value: '"unhealthy_endpoints"'
      - type: body.contains
        not: true
        value: "<!DOCTYPE"
      - type: body.contains
        not: true
        value: "<html"
    confidence: high

  - type: http
    path: /health
    method: GET
    match:
      - type: status
        value: 200
      - type: body.contains
        value: '"litellm_metadata"'
      - type: body.contains
        not: true
        value: "<!DOCTYPE"
    confidence: high

  - type: http
    path: /health/liveliness
    method: GET
    match:
      - type: status
        value: 200
      - type: body.contains
        value: '"status"'
      - type: body.contains
        not: true
        value: "<!DOCTYPE"
    confidence: medium

  - type: http
    path: /health/readiness
    method: GET
    match:
      - type: status
        value: 200
      - type: body.contains
        value: '"status"'
      - type: body.contains
        not: true
        value: "<!DOCTYPE"
    confidence: medium

models:
  path: /v1/models
  method: GET
  extract: ".data[].id"

augustus:
  generator: openai
  config_template:
    endpoint: "$TARGET/v1"
    model: "$MODEL"
    api_key: "$API_KEY"
    timeout: 180
