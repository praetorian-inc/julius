# probes/ollama.yaml
# Reference: https://github.com/ollama/ollama/blob/main/docs/api.md
name: ollama
description: Ollama local LLM server
category: self-hosted
port_hint: 11434
specificity: 100
api_docs: https://github.com/ollama/ollama/blob/main/docs/api.md

requests:
  # Primary: Definitive "Ollama is running" message (highest confidence)
  - type: http
    path: /
    method: GET
    match:
      - type: status
        value: 200
      - type: body.contains
        value: "Ollama is running"

  # Secondary: /api/tags with Ollama-specific family markers
  # Note: KoboldCpp also exposes /api/tags but with "family": "koboldcpp"
  - type: http
    path: /api/tags
    method: GET
    match:
      - type: status
        value: 200
      - type: content-type
        value: application/json
      - type: body.contains
        value: '"models"'
      - type: body.contains
        value: '"family":'
      # Exclude KoboldCpp which mimics Ollama's /api/tags endpoint
      - type: body.contains
        value: '"family": "koboldcpp"'
        not: true

models:
  path: /api/tags
  method: GET
  extract: ".models[].name"

augustus:
  generator: ollama
  config_template:
    endpoint: "$TARGET"
    model: "$MODEL"
    timeout: 180
